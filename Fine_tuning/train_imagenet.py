#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import torchvision
import torchvision.transforms as T
import torch.nn as nn
import torch.backends.cudnn as cudnn
import os
import numpy as np
import argparse
from os.path import dirname
from numpy import linalg as la
import logging
from collections import OrderedDict

# from thop import profile
import time, datetime
import utils.common as utils
import utils.als_process as als_process

from models.vgg_ori import vgg_16_bn
from models.vgg_debut import vgg16bn_debut
# from models.vgg_fastfood import VGG_Fastfood
from models.vgg_pytorch import vgg16_bn

from models.resnet import resnet50

from data import imagenet

# Parse args
def FLAGS():
    parser = argparse.ArgumentParser("""Fine-tuning of DeBut with or without ALS initialization""")

    parser.add_argument(
        "--log_dir", 
        default="", 
        required = True,
        help='Log directory')
    parser.add_argument(
        '--r_shape_txt',
        type=str,
        help='r shape file for vgg'
    )
    parser.add_argument(
        '--arch',
        type=str,
        help='architecture')
    parser.add_argument(
        '--data_dir',
        type=str,
        default='',
        help='path to dataset')

    # ALS params
    parser.add_argument(
        '--use_ALS',
        action='store_true',
        help='whether to use ALS method as the pretrained model')
    parser.add_argument(
        '--DeBut_init_dir',
        type=str,
        help='path to the debut factors generated by ALS')
    parser.add_argument(
        '--pretrained_file',
        type=str,
        help='path to the pretrained checkpoint')
    parser.add_argument(
        '--debut_layers',
        nargs='+',
        type=str,
        help='which layer to use ALS'
    )

    # training params
    parser.add_argument(
        '--lr_type',
        default='step',
        type=str,
        help='learning rate decay schedule')

    parser.add_argument(
        '--batch_size',
        type=int,
        default=256,
        help='batch size')
    parser.add_argument(
        '--epochs',
        type=int,
        default=150,
        help='num of training epochs')

    parser.add_argument(
        '--learning_rate',
        type=float,
        default=0.01,
        help='init learning rate')

    parser.add_argument(
        '--momentum',
        type=float,
        default=0.9,
        help='momentum')

    parser.add_argument(
        '--weight_decay',
        type=float,
        default=5e-4,
        help='weight decay')

    parser.add_argument(
        '--label_smooth',
        type=float,
        default=0.1,
        help='label smoothing')

    # test mode
    parser.add_argument(
        '--test_only',
        action='store_true',
        help='whether to use the test mode only')

    parser.add_argument(
        '--test_model_dir',
        type=str,
        default='',
        help='test model path')

    parser.add_argument(
        '--resume',
        action='store_true',
        help='whether continue training from the pretrained net')

    parser.add_argument(
        '--gpu',
        type=str,
        default='0',
        help='Choose gpu to use'
    )
    flags = parser.parse_args()

    return flags

flags = FLAGS()

os.environ['CUDA_VISIBLE_DEVICES'] = flags.gpu

if not os.path.isdir(flags.log_dir):
    os.makedirs(flags.log_dir)

CLASSES = 1000
print_freq = 128000//flags.batch_size

utils.record_config(flags)
now = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')
logger = utils.get_logger(os.path.join(flags.log_dir, 'logger'+now+'.log'))
    
if len(flags.gpu)>1:
    name_base='module.'
else:
    name_base=''

# Process the R chains
def process_chain_params(r_shapes):
    mat_shapes = r_shapes
    R_mats = []
    num_mat = len(r_shapes) // 5
    for i in range(num_mat):
        start_i, end_i = i * 5, (i + 1) * 5
        R_mat = mat_shapes[start_i: end_i]
        R_mats.append(R_mat)
    return R_mats

def process_chain_file(r_shapes_file):
    R_shapes = []
    f = open(r_shapes_file, 'r')
    while True:
        line = f.readline()
        if len(line) <= 1:
            break
        temp = line.strip('\n').split(' ')
        r_shape = [int(a) for a in temp]
        R_shape = process_chain_params(r_shape)
        R_shapes.append(R_shape)
    f.close()
    return R_shapes

def process_debut_filenames(DeBut_init_dir,suffix):
    path_dir = DeBut_init_dir
    prefix = '/vgg_als3_conv'
    debut_filenames = []
    for i in range(len(suffix)):
        debut_filenames.append(path_dir + prefix + str(suffix[i]) + '/Best_DeBut_info.pth.tar')
    return debut_filenames

def adjust_learning_rate(optimizer, epoch, step, len_iter):

    if flags.lr_type == 'step':
        factor = epoch // 30
        if epoch >= 80:
            factor = factor + 1
        lr = flags.learning_rate * (0.1 ** factor)

    elif flags.lr_type == 'cos':  # cos without warm-up
        lr = 0.5 * flags.learning_rate * (1 + math.cos(math.pi * (epoch - 5) / (args.epochs - 5)))

    elif flags.lr_type == 'exp':
        step = 1
        decay = 0.96
        lr = flags.learning_rate * (decay ** (epoch // step))

    elif flags.lr_type == 'fixed':
        lr = flags.learning_rate
    else:
        raise NotImplementedError

    #Warmup
    if epoch < 5:
        lr = lr * float(1 + step + epoch * len_iter) / (5. * len_iter)

    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

    if step == 0:
        logger.info('learning_rate: ' + str(lr))


def main():
    start_t = time.time()

    cudnn.benchmark = True
    cudnn.enabled=True
  
    logger.info("args = %s", flags)
    logger.info('-' * 30)

    torch.cuda.is_available()
    
    if flags.r_shapes:
        R_shapes = process_chain_params(flags.r_shapes)
        logger.info('DeBut matrices shapes are:' + str(R_shapes))
    elif flags.r_shape_txt:
        R_shapes = process_chain_file(flags.r_shape_txt)
        logger.info('DeBut matrices shapes are:' + str(R_shapes))
 
    # load mode
    logger.info('==> Building model...')
  
    model = eval(flags.arch)(R_shapes = R_shapes)
    model = model.cuda()
    logger.info(model)

    logger.info("Num of parameters = " + str(sum([p.numel() for p in model.parameters()]))) # print the structure of the model
    for name, param in model.named_parameters():
        logger.info("Num of parameters of {} = {}".format(name, param.shape))
    
    # load training data
    data_tmp = imagenet.Data(flags)
    train_loader = data_tmp.train_loader
    val_loader = data_tmp.test_loader

    criterion = nn.CrossEntropyLoss()
    criterion = criterion.cuda()
    criterion_smooth = utils.CrossEntropyLabelSmooth(CLASSES, flags.label_smooth)
    criterion_smooth = criterion_smooth.cuda()

    # Test the trained model
    if flags.test_only:
        val_loss, val_top1_acc, valid_top5_acc= test(0, val_loader, model, criterion, flags)
        return
        if os.path.isfile(flags.test_model_dir):
            logger.info('Loading checkpoint {} ......'.format(flags.test_model_dir))
            checkpoint = torch.load(flags.test_model_dir)

            new_state_dict = OrderedDict()
            for k, v in checkpoint['state_dict'].items():
                new_state_dict[k.replace('module', '')] = v
            # new_state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}
            model.load_state_dict(new_state_dict)
            val_loss, val_top1_acc, valid_top5_acc= test(0, val_loader, model, criterion, flags)
        else:
            logger.info('please specify a checkpoint file')
        return

    if len(flags.gpu) > 1:
        logger.info('Using {} gpus'.format(flags.gpu))
        device_id = []
        for i in range((len(flags.gpu) + 1) // 2):
            device_id.append(i)
        model = nn.DataParallel(model, device_ids=device_id).cuda()
    
    # training parameters
    optimizer = torch.optim.SGD(model.parameters(), lr=flags.learning_rate, momentum=flags.momentum, weight_decay=flags.weight_decay)

    # training
    start_epoch = 0
    best_top1_acc = 0 
    best_top5_acc= 0

    if flags.use_ALS:
        logger.info('resuming from pretrained model and load debut vectors')

        ori_state_dict = model.state_dict()
        ckpt = torch.load(flags.pretrained_file)
        if 'state_dict' in ckpt:
            regular_weights = ckpt['state_dict']
        else:
            regular_weights = ckpt
  
        # Replace except debut layers
        new_state_dict = OrderedDict()
        for k, v in regular_weights.items():
            if name_base + k in ori_state_dict:
                new_state_dict[name_base + k] = v 
        for k, v in new_state_dict.items():
            logger.info('Replace {} with the pretrained checkpoint'.format(k))
        ori_state_dict.update(new_state_dict)
        model.load_state_dict(ori_state_dict)
     
        # Replace the debut layers
        elif flags.arch == 'resnet50':
            debut_layers = ['layer4_0_conv1', 'layer4_0_conv2', 'layer4_0_conv3', 'layer4_1_conv1', 'layer4_1_conv2', 'layer4_1_conv3', 
                            'layer4_2_conv1', 'layer4_2_conv2', 'layer4_2_conv3']
            for i, layer in enumerate(debut_layers):
                als_file_name = layer
                als_file_path = flags.DeBut_init_dir + layer + '/Best_DeBut_info.pth.tar'
                ckpt_factors = torch.load(als_file_path)
                DeBut_factors = ckpt_factors['DeBut_factors']
             
                DeBut_vec = als_process.process_als_weight(DeBut_factors, R_shapes[i])
                DeBut_vec_params = nn.Parameter(torch.tensor(DeBut_vec))
                logger.info('DeBut vector{} shape: {}'.format(i+1, DeBut_vec_params.shape))

                sub_layer_name = layer.split('_')[0]
                block_index = layer.split('_')[1]
                conv_name = layer.split('_')[2]

                module = getattr(model, 'module')
                sub_layers = getattr(module, sub_layer_name)
                block = getattr(sub_layers, block_index)
                conv = getattr(block, conv_name)
                setattr(conv, 'twiddle', DeBut_vec_params)

        model = model.cuda()

    if flags.resume:
        checkpoint_dir = os.path.join(flags.log_dir, 'checkpoint.pth.tar')

        logger.info('loading checkpoint {} ..........'.format(checkpoint_dir))
        checkpoint = torch.load(checkpoint_dir)
        start_epoch = checkpoint['epoch']+1
        best_top1_acc = checkpoint['best_top1_acc']
        if 'best_top5_acc' in checkpoint:
            best_top5_acc = checkpoint['best_top5_acc']

        #deal with the single-multi GPU problem
        new_state_dict = OrderedDict()
        tmp_ckpt = checkpoint['state_dict']
        if len(flags.gpu) > 1:
            for k, v in tmp_ckpt.items():
                new_state_dict['module.' + k.replace('module.', '')] = v
        else:
            for k, v in tmp_ckpt.items():
                new_state_dict[k.replace('module.', '')] = v

        model.load_state_dict(new_state_dict)
        logger.info("loaded checkpoint {} epoch = {}".format(checkpoint_dir, checkpoint['epoch']))

    epoch = start_epoch
    while epoch < flags.epochs:

        train_obj, train_top1_acc,  train_top5_acc = train(epoch,  train_loader, model, criterion_smooth, optimizer)
        valid_obj, valid_top1_acc, valid_top5_acc = test(epoch, val_loader, model, criterion, flags)
        
        is_best = False
        if valid_top1_acc > best_top1_acc:
            best_top1_acc = valid_top1_acc
            best_top5_acc = valid_top5_acc
            is_best = True

        utils.save_checkpoint({
            'epoch': epoch,
            'state_dict': model.state_dict(),
            'best_top1_acc': best_top1_acc,
            'best_top5_acc': best_top5_acc,
            'optimizer' : optimizer.state_dict(),
            }, is_best, flags.log_dir)

        epoch += 1
        logger.info("=>Best accuracy Top1: {:.3f}, Top5: {:.3f}".format(best_top1_acc, best_top5_acc))

    training_time = (time.time() - start_t) / 36000
    logger.info('total training time = {} hours'.format(training_time))

def train(epoch, train_loader, model, criterion, optimizer):
    batch_time = utils.AverageMeter('Time', ':6.3f')
    data_time = utils.AverageMeter('Data', ':6.3f')
    losses = utils.AverageMeter('Loss', ':.4e')
    top1 = utils.AverageMeter('Acc@1', ':6.2f')
    top5 = utils.AverageMeter('Acc@5', ':6.2f')

    model.train()
    end = time.time()

    num_iter = len(train_loader)
    print_freq = num_iter // 10

    for batch_idx, (data, target) in enumerate(train_loader):
        
        data = data.cuda()
        target = target.cuda()
        data_time.update(time.time() - end)

        adjust_learning_rate(optimizer, epoch, batch_idx, num_iter)

        # output and loss
        output = model(data)
        loss = criterion(output, target)

        # measure accuracy and record loss
        prec1, prec5 = utils.accuracy(output, target, topk=(1, 5))
        n = data.size(0)
        losses.update(loss.item(), n)  # accumulated loss
        top1.update(prec1.item(), n)
        top5.update(prec5.item(), n)

        # gradient and optimizer
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # measure elapsed time
        batch_time.update(time.time() - end)
        end = time.time()

        if batch_idx % print_freq == 0 and batch_idx != 0:
            logger.info(
                'Epoch[{0}]({1}/{2}): '
                'Loss {loss.avg:.4f} '
                'Prec@1(1,5) {top1.avg:.2f}, {top5.avg:.2f}'.format(
                    epoch, batch_idx, num_iter, loss=losses,
                    top1=top1, top5=top5))

    return losses.avg, top1.avg, top5.avg

@torch.no_grad()
def test(epoch, val_loader, model, criterion, flags):
    batch_time = utils.AverageMeter('Time', ':6.3f')
    losses = utils.AverageMeter('Loss', ':.4e')
    top1 = utils.AverageMeter('Acc@1', ':6.2f')
    top5 = utils.AverageMeter('Acc@5', ':6.2f')

    model.eval()
    val_loss, correct = 0, 0
    end = time.time()
    for batch_idx, (data, target) in enumerate(val_loader):
        data = data.cuda()
        target = target.cuda()
        output = model(data)
        loss = criterion(output, target)

        pred1, pred5 = utils.accuracy(output, target, topk=(1, 5)) # get the index of the max log-probability
        n = data.size(0)
        losses.update(loss.item(), n)
        top1.update(pred1[0], n)
        top5.update(pred5[0], n)

        batch_time.update(time.time() - end)
        end = time.time()

    logger.info(' * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'
                    .format(top1=top1, top5=top5))

    return losses.avg, top1.avg, top5.avg

if __name__ == '__main__':
    main()
 