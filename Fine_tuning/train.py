#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import torchvision
import torchvision.transforms as T
import torch.nn as nn
import torch.backends.cudnn as cudnn
import os
import numpy as np
import argparse
from os.path import dirname
from numpy import linalg as la
import logging
from collections import OrderedDict

import time, datetime
import utils.common as utils
import utils.als_process as als_process

# DeBut models
from models.lenet_debut import LeNet_DeBut
from models.vgg_debut import VGG_DeBut

# Parse args
def FLAGS():
    parser = argparse.ArgumentParser("""Fine-tuning of DeBut with or without ALS initialization""")

    parser.add_argument(
        "--log_dir", 
        default="", 
        required = True,
        help='log directory')
    parser.add_argument(
        "--data_dir", 
        default="", 
        required = True,
        help='data directory')
    parser.add_argument(
        '--r_shape_txt',
        type=str,
        required = True,
        help='chains shape file'
    )
    parser.add_argument(
        '--arch',
        type=str,
        help='which model to use')
    parser.add_argument(
        '--dataset',
        type=str,
        help='training dataset'
    )

    # ALS params
    parser.add_argument(
        '--use_ALS',
        action='store_true',
        help='whether to use ALS method as the pretrained model')
    parser.add_argument(
        '--use_pretrain',
        action='store_true',
        help='whether to use the pretrained model')
    parser.add_argument(
        '--DeBut_init_dir',
        type=str,
        help='path to the debut factors generated by ALS')
    parser.add_argument(
        '--pretrained_file',
        type=str,
        help='path to the pretrained checkpoint')
    parser.add_argument(
        '--debut_layers',
        nargs='+',
        type=int,
        help='which layers to use ALS'
    )

    # training params
    parser.add_argument(
        '--batch_size',
        type=int,
        default=256,
        help='batch size')
    parser.add_argument(
        '--optimizer',
        type=str,
        default='sgd',
        help='optimizer to update parameters')
    parser.add_argument(
        '--epochs',
        type=int,
        default=150,
        help='num of training epochs')

    parser.add_argument(
        '--learning_rate',
        type=float,
        default=0.01,
        help='init learning rate')

    parser.add_argument(
        '--lr_decay_step',
        default='50,100',
        type=str,
        help='learning rate decay step')

    parser.add_argument(
        '--momentum',
        type=float,
        default=0.9,
        help='momentum')

    parser.add_argument(
        '--weight_decay',
        type=float,
        default=5e-4,
        help='weight decay')

    # test mode
    parser.add_argument(
        '--test_only',
        action='store_true',
        help='whether to use the test mode only')

    parser.add_argument(
        '--test_model_dir',
        type=str,
        default='',
        help='test model path')

    parser.add_argument(
        '--resume',
        action='store_true',
        help='whether to continue training from the pretrained net')

    parser.add_argument(
        '--gpu',
        type=str,
        default='0',
        help='Choose gpu to use'
    )
    flags = parser.parse_args()

    return flags

flags = FLAGS()

os.environ['CUDA_VISIBLE_DEVICES'] = flags.gpu

utils.record_config(flags)
now = datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')
logger = utils.get_logger(os.path.join(flags.log_dir, 'logger'+now+'.log'))
    
# Process the R chains
def process_chain_params(r_shapes):
    mat_shapes = r_shapes
    R_mats = []
    num_mat = len(r_shapes) // 5
    for i in range(num_mat):
        start_i, end_i = i * 5, (i + 1) * 5
        R_mat = mat_shapes[start_i: end_i]
        R_mats.append(R_mat)
    return R_mats

def process_chain_file(r_shapes_file):
    R_shapes = []
    f = open(r_shapes_file, 'r')
    while True:
        line = f.readline()
        if len(line) <= 1:
            break
        temp = line.strip('\n').split(' ')
        r_shape = [int(a) for a in temp]
        R_shape = process_chain_params(r_shape)
        R_shapes.append(R_shape)
    f.close()
    return R_shapes

def process_debut_filenames(DeBut_init_dir, suffix, prefix):
    path_dir = DeBut_init_dir
    debut_filenames = []
    for i in range(len(suffix)):
        debut_filenames.append(path_dir + prefix + str(suffix[i]) + '/Best_DeBut_info.pth.tar')
    return debut_filenames

def main():
    cudnn.benchmark = True
    cudnn.enabled=True
    # log path
    if not os.path.exists(flags.log_dir):
        os.mkdir(flags.log_dir)

    logger.info("args = %s", flags)
    logger.info('-' * 30)

    R_shapes = process_chain_file(flags.r_shape_txt)
    logger.info('DeBut matrices shapes are:' + str(R_shapes))

    # load mode
    logger.info('==> Building model...')
    model = eval(flags.arch)(R_shapes = R_shapes, DeBut_layer = flags.debut_layers).cuda()
    logger.info(model)

    logger.info("Num of parameters = " + str(sum([p.numel() for p in model.parameters()]))) 
    for name, param in model.named_parameters():
        logger.info("Num of parameters of {} = {}".format(name, param.shape))
    
    # load training data
    if flags.dataset == 'MNIST':
        dataset = torchvision.datasets.MNIST(root=flags.data_dir, 
                                        train=True, 
                                        transform=torchvision.transforms.ToTensor(), 
                                        download=True)
        test_dataset = torchvision.datasets.MNIST(root=flags.data_dir, 
                                        train=False, 
                                        transform=torchvision.transforms.ToTensor(), 
                                        download=True) 
    elif flags.dataset == 'CIFAR10':
        dataset = torchvision.datasets.CIFAR10( root=flags.data_dir, train=True, transform=T.Compose([
                    T.RandomCrop(32, padding=4),
                    T.RandomHorizontalFlip(),
                    T.ToTensor(),
                    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
                ]), download=True
            )
        test_dataset = torchvision.datasets.CIFAR10(
                root=flags.data_dir, train=False, transform=T.Compose([
                    T.ToTensor(),
                    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
                ]), download=True
            )
    data_loader = torch.utils.data.DataLoader(dataset, batch_size = flags.batch_size, num_workers=8, shuffle=True)
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = flags.batch_size, num_workers=8)

    criterion = nn.CrossEntropyLoss()
    criterion = criterion.cuda()
    # Test the trained model
    if flags.test_only:
        if os.path.isdir(flags.test_model_dir):
            logger.info('Loading checkpoint {} ......'.format(flags.test_model_dir))
            checkpoint = torch.load(flags.test_model_dir + '/model_best.pth')
            
            new_state_dict = OrderedDict()
            if len(flags.gpu) > 1:
                for k, v in checkpoint['state_dict'].items():
                    new_state_dict['module.' + k.replace('module.', '')] = v
            else:
                for k, v in checkpoint['state_dict'].items():
                    new_state_dict[k.replace('module.', '')] = v
          
            model.load_state_dict(new_state_dict)
            
            start_time = time.time()
            val_loss, val_prec1 = test(0, test_loader, model, criterion, flags)
            end_time = time.time()
            logger.info('Inference time: {}'.format(end_time - start_time))
        else:
            logger.info('please specify a checkpoint file')
        return

    if len(flags.gpu) > 1:
        device_id = []
        for i in range((len(flags.gpu) + 1) // 2):
            device_id.append(i)
        model = nn.DataParallel(model, device_ids=device_id).cuda()
    
    # training parameters
    if flags.optimizer == 'adam':
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)
    elif flags.optimizer == 'sgd':
        optimizer = torch.optim.SGD(model.parameters(), lr=flags.learning_rate, momentum=flags.momentum, weight_decay=flags.weight_decay)
    lr_decay_step = list(map(int, flags.lr_decay_step.split(',')))
    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=lr_decay_step, gamma=0.1)
    
    # training
    best_top1_acc = 0 

    if flags.use_pretrain:
        # load the pretrained model
        logger.info('resuming from pretrained model but not load debut vectors')
        ori_state_dict = model.state_dict()
        ckpt = torch.load(flags.pretrained_file)
        regular_weights = ckpt['state_dict']

        new_state_dict = OrderedDict()
        if len(flags.gpu) > 1:
            for k, v in regular_weights.items():
                if 'module.' + k.replace('module.', '') in ori_state_dict:
                    new_state_dict['module.' + k.replace('module.', '')] = v
        else:
            for k, v in regular_weights.items():
                if k.replace('module.', '') in ori_state_dict:
                    new_state_dict[k.replace('module.', '')] = v
        
        ori_state_dict.update(new_state_dict)
        model.load_state_dict(ori_state_dict)
        
        # Load ALS init files
    if flags.use_ALS:
        debut_layers = flags.debut_layers
        if flags.arch == 'VGG_DeBut':
            debut_layer_names = ['debut' + str(i) for i in debut_layers]
            als_init_files = process_debut_filenames(flags.DeBut_init_dir, debut_layers, prefix = '/vgg_als3_conv')

            if len(flags.gpu) > 1:
                module = getattr(model, 'module')
                features = getattr(module, 'features')
            else:
                features = getattr(model, 'features')
            for idx, als_file in enumerate(als_init_files):
                logger.info('Loading debut factors from {}......'.format(als_file))
                ckpt_factors = torch.load(als_file)
                DeBut_factors = ckpt_factors['DeBut_factors']
                DeBut_vec = als_process.process_als_weight(DeBut_factors, R_shapes[idx])
                DeBut_vec_params = nn.Parameter(torch.tensor(DeBut_vec))
                logger.info('DeBut vector{} shape: {}'.format(idx+1, DeBut_vec_params.shape))

                # Replace debut layers
                if getattr(features, debut_layer_names[idx]):
                    layer_name = getattr(features, debut_layer_names[idx])
                    # set parameters for DeBut twiddle
                    setattr(layer_name, 'twiddle', DeBut_vec_params)  

        elif flags.arch == 'LeNet_DeBut':
            debut_layer_names = []
            debut_R_shapes = []
            for i in debut_layers:
                if i == 0:
                    debut_layer_names.append('conv2')
                    debut_R_shapes.append(R_shapes[i])
                elif i == 1:
                    debut_layer_names.append('fc')
                    debut_R_shapes.append(R_shapes[i])
                elif i == 2:
                    debut_layer_names.append('fc2')
                    debut_R_shapes.append(R_shapes[i])
            
            als_init_files = process_debut_filenames(flags.DeBut_init_dir, debut_layer_names, prefix = '/lenet_')
            DeBut_vec_params_list = []
            for idx, als_file in enumerate(als_init_files):
                logger.info('Loading debut factors from {}......'.format(als_file))
                ckpt_factors = torch.load(als_file)
                DeBut_factors = ckpt_factors['DeBut_factors']
                DeBut_vec = als_process.process_als_weight(DeBut_factors, debut_R_shapes[idx])
                DeBut_vec_params = nn.Parameter(torch.tensor(DeBut_vec))
                logger.info('DeBut vector{} shape: {}'.format(idx+1, DeBut_vec_params.shape))
                DeBut_vec_params_list.append(DeBut_vec_params)
                
                if getattr(model, debut_layer_names[idx]):
                    layer_name = getattr(model, debut_layer_names[idx])
                    setattr(layer_name, 'twiddle', DeBut_vec_params)  
                else:
                    raise SystemExit 
    model = model.cuda()
    time_start=time.time()

    for epoch in range(flags.epochs):
        fine_tune(epoch, model, data_loader, criterion, optimizer, lr_scheduler, log_interval = 200)
        val_loss, val_prec1 = test(epoch, test_loader, model, criterion, flags)

        if val_prec1 > best_top1_acc:
            best_top1_acc = val_prec1
            state_dict = model.state_dict()

            torch.save({
                "state_dict": state_dict,
                "min_val_loss": val_loss 
            }, "%s/model_best.pth" % flags.log_dir)

    time_end=time.time()
    logger.info('time cost:' + str(time_end-time_start) + 's')

def fine_tune(epoch, model, data_loader, criterion, optimizer, lr_scheduler, log_interval = 200):
    model.train()
    for param_group in optimizer.param_groups:
        cur_lr = param_group['lr']
    logger.info('learning_rate: ' + str(cur_lr))
    
    for batch_idx, (data, target) in enumerate(data_loader):
        
        data = data.cuda()
        target = target.cuda()
        output = model(data)
        
        loss = criterion(output, target)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch_idx % log_interval == 0:
            logger.info('Train Epoch: {} [{:0>5d}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                    epoch+1, batch_idx * len(data), len(data_loader.dataset),
                    100. * batch_idx * len(data) / len(data_loader.dataset), loss.item()))
    lr_scheduler.step()

@torch.no_grad()
def test(epoch, test_loader, model, criterion, flags):
    model.eval()
    val_loss, correct = 0, 0
    for batch_idx, (data, target) in enumerate(test_loader):
        data = data.cuda()
        target = target.cuda()
        output = model(data)
        val_loss += criterion(output, target).item()
        pred = output.data.max(1)[1] # get the index of the max log-probability
        correct += pred.eq(target).cpu().sum()
    
    val_loss =  val_loss/len(test_loader)

    accuracy = 100. * correct.to(torch.float32) / len(test_loader.dataset)
  
    logger.info('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        val_loss, correct, len(test_loader.dataset), accuracy))
    return val_loss, accuracy

if __name__ == '__main__':
    main()
 